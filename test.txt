\documentclass{article}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% ICML 2025 STYLE SETUP
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% TEST
\usepackage{enumitem}

% hyperref makes hyperlinks in the resulting PDF.
\usepackage{hyperref}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the camera-ready submission:
%\usepackage[accepted]{icml2025}
\usepackage{icml2025}

% For theorems and such
\usepackage{amsmath,amssymb,amsthm}
\usepackage{mathtools}
\usepackage{todonotes}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}

\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}

\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Title and Author Setup
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\icmltitlerunning{Position: A Theory of Deep Learning must include Compositional Sparsity}

\begin{document}

\twocolumn[
\icmltitle{Position: A Theory of Deep Learning must include Compositional Sparsity}

\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Davide D'Ascenzo}{equal,mit,torino}
\icmlauthor{Rafael Dubach}{equal,mit,uzh}
\icmlauthor{Tomaso Poggio}{equal,mit}
\end{icmlauthorlist}

\icmlaffiliation{mit}{Center for Brains, Minds and Machines (CBMM), MIT, Cambridge, MA, USA}
\icmlaffiliation{uzh}{University of Zurich, Zurich, Switzerland}
\icmlaffiliation{torino}{Politecnico di Torino, Torino, Italy}

\icmlcorrespondingauthor{Davide D'Ascenzo}{kidara@mit.edu}
\icmlcorrespondingauthor{Rafael Dubach}{raduba@mit.edu}
\icmlcorrespondingauthor{Tomaso Poggio}{tp@mit.edu}

\vskip 0.3in
]

\printAffiliationsAndNotice{}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% ABSTRACT
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{abstract}
Neural networks have demonstrated impressive success in various domains, raising the question of what fundamental principles underlie the effectiveness of the best AI systems and quite possibly of human intelligence. \textbf{This position paper} argues that \emph{compositional sparsity}, the property that a compositional function has “few” constituent functions, each depending on only a small subset of inputs, is a key principle underlying successful learning architectures. Surprisingly, \emph{all} functions that are efficiently Turing-computable admit a compositionally sparse representation. This implies that deep and sparse neural networks are a good choice to learn from examples in a classical ERM framework, since they are a parametric representation that requires at most a polynomial number of parameters for a large class of functions (class $P$ in the Boolean case). The property of compositional sparsity explains why depth and sparsity are important in learning, why neural networks do not suffer the curse of dimensionality and why convolutional networks are better than dense networks. Finally, intriguing \emph{learnability} questions remain, suggesting that other properties, complementing compositional sparsity, need to be characterized for a more satisfying learning theory.
\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% INTRODUCTION
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}
The current era is marked by the emergence of powerful deep learning architectures that are transforming industries and society. Yet, we still lack a fundamental understanding of why deep networks work and what their limitations are. It was a similar situation with electricity after Volta’s invention of the \emph{pila} opened the way to the telegraph, the electric motor and electric generators despite the lack of a theory, which was finally achieved by Maxwell in 1864.

In the field of deep learning we are between Volta and Maxwell: we still need to achieve a fundamental theoretical understanding of deep learning, its architectures and algorithms. There are many reasons why this is important. Present attempts to understand, predict and control behaviors of machine learning models are not based on fundamental principles and are unlikely to be successful\todo{Is this claim too strong?}. On the other hand, there is a real hope that such fundamental principles may exist and can be discovered as shown by recent progress in the theory of deep learning.

Such hope is indeed motivated by the impressive achievements of deep learning in various domains. AlphaFold has revolutionized protein structure prediction \cite{alphafold_1,alphafold_2}, large language models like GPT-3 have demonstrated impressive natural language generation capabilities \cite{gpt3}, and AlphaGeometry has been able to solve International Mathematical Olympiad problems \cite{alphageometry}.

These successes have sparked intense interest in understanding the theoretical principles underlying such versatile capabilities. Why do neural networks, in particular, \emph{deep} architectures, often scale so effectively to high-dimensional tasks?

Historically, attempts to build intelligent machines \cite{McCarthy_1955} often emphasized \emph{rule-based} or \emph{expert systems}. In contrast, modern breakthroughs primarily stem from learning architectures, especially multi-layered neural networks. This shift has prompted renewed interest in learning theory foundations, focusing on three core questions:
\begin{enumerate}
    \item \textbf{Approximation capacity:} How well can neural networks approximate functions of interest?
    \item \textbf{Optimization:} How can we train (i.e., optimize) network parameters on finite training data?
    \item \textbf{Generalization:} Why do networks not overfit, or at least, how do they still perform well despite their large capacity?
\end{enumerate}

\noindent
\textbf{Our position is that compositional sparsity is a key principle in explaining why deep networks can scale to high-dimensional tasks efficiently}. In this paper, we focus primarily on approximation and optimization but also highlight connections to generalization. We argue, building on \citet{poggiofraser2024}, that compositional sparsity:

\begin{itemize}
    \item Explains how deep networks can approximate a very large class of functions (namely, all efficiently Turing-computable functions) \emph{without} requiring an exponential number of parameters;
    \item Suggests that discovering and training such representations can be challenging, yet potentially more tractable when the compositional structure is partially revealed (e.g., via chain-of-thought, or by imposing architectural constraints).
\end{itemize}

The paper is structured as follows. In \cref{sec:preliminaries}, we provide a brief overview of learning theory and the importance of the hypothesis class $\mathcal{H}$. In \cref{sec:shallow-deep}, we discuss the limitations of shallow networks and the advantages of deep architectures. In \cref{sec:compositional_sparsity}, we introduce the concept of compositional sparsity and its implications for deep learning. In \cref{sec:proofsketch}, we provide a more detailed proof sketch of why efficiently computable functions are compositionally sparse. In \cref{sec:approximation}, we show how deep networks can then approximate such functions with polynomial capacity. In \cref{sec:learnability}, we discuss the challenges of learning compositional functions and conjectural perspectives on how to address them. In \cref{sec:alternativeviews}, we present alternative viewpoints, and finally, we conclude with a discussion in \cref{sec:discussion}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Preliminaries in Learning Theory}
\label{sec:preliminaries}
We begin by briefly reviewing the essential setup of supervised learning, as in \citet{vapnik2013nature,hastie2009elements,james2023introduction}.

A supervised learning problem is defined by an unknown probability distribution $\mu$ on $\mathcal{X} \times \mathcal{Y}$, from which one draws an i.i.d.~training set $S = \{(x_i, y_i)\}_{i=1}^m$. If one views $y_i$ as a (possibly noisy) label associated with input $x_i$, the underlying goal is to approximate the \emph{target function} $f_\mu \colon \mathcal{X} \to \mathcal{Y}$ that best explains the data in some sense, typically measured by the expected loss
\[
\mathcal{L}(f) \;=\; \int (f(x) - y)^2 \, d\mu(x,y)
\]
(for the squared error loss, for instance). Since $\mu$ is unknown, one instead performs \emph{empirical risk minimization} over a chosen \emph{hypothesis class} $\mathcal{H}$:
\[
f_S \;=\; \arg\min_{f \in \mathcal{H}} \Bigl\{\tfrac{1}{m}\!\sum_{i=1}^m (f(x_i) - y_i)^2 \;+\; \text{regularizer}(f)\Bigr\},
\]
aiming to ensure that $f_S$ also performs well on unseen data (generalization).

From an approximation-theoretic perspective, the choice of $\mathcal{H}$ is crucial. If $\mathcal{H}$ is too narrow, it may not approximate $f_\mu$ well; if it is too large, searching it might be intractable or lead to poor generalization. \emph{Deep neural networks} (DNNs) have emerged as a surprisingly effective balance between expressiveness and search, but a core question remains: \emph{Which property of real-world target functions, $f_\mu$, ensures that a suitable deep network in $\mathcal{H}$ can approximate it with polynomially many parameters?}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Shallow vs. Deep Approximation and the Curse of Dimensionality}
\label{sec:shallow-deep}
Classical approximation theory has shown that \emph{shallow} networks (i.e., one-hidden-layer networks) can approximate any continuous function arbitrarily well on a compact set \cite{Pinkus_1999}. However, the number of parameters can grow \emph{exponentially} in the input dimension $d$. This well-known phenomenon is referred to as the \emph{curse of dimensionality} \cite{Bellman1957}.

\paragraph{Shallow Approximation Bound.}
The limitations of shallow networks become apparent when considering the approximation of functions in Sobolev or H\"older classes, denoted as $\mathrm{W}_{s}^d$. For such function classes, a shallow network can achieve an $\epsilon$-approximation (in uniform or $L^2$ norm) only if its size $N$ satisfies roughly $N = \mathcal{O}(\epsilon^{-d/s})$, which becomes infeasible as $d$ grows \cite{Pinkus_1999}. The smoothness parameter $s$ partially mitigates this effect, but the curse of dimensionality persists.

\paragraph{When is Deep Better?}
A growing body of work \cite{MhaskarPoggio2016b,dahmen_compositional_2023, cagnetta_compositional_2024, poggiofraser2024} has demonstrated that if the target function $f(x_1, \dots, x_d)$ exhibits a \emph{compositional} structure, then a \emph{deep} network that mirrors this structure can break the exponential dependence on $d$. This compositional property allows the function $f$ to be decomposed into a hierarchy of subfunctions, each depending on only a small subset of variables. These subfunctions are then composed in a nested manner, creating a multi-level structure. Such an arrangement enables deep networks to exploit this inherent hierarchy, leading to more efficient function approximation. This efficiency stems from the network’s ability to learn and represent these subfunctions at different levels of abstraction, effectively breaking down the complex target function into more manageable components. Consequently, deep architectures can achieve superior approximation capabilities compared to their shallow counterparts for a wide class of practically relevant functions, particularly in high-dimensional spaces.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Compositional Sparsity}
\label{sec:compositional_sparsity}

\subsection{Definition and Main Theorem}
\begin{definition}[Sparse Compositional Function]
\label{def:sparse_comp}
A function $f:\mathbb{R}^d \rightarrow \mathbb{R}^k$ is \emph{compositionally sparse} if there exists a directed acyclic graph (DAG) $\mathcal{G}$ of polynomial size in $d$, whose internal nodes each have in-degree at most $c$ (a small constant), such that $f$ factors as a composition of node functions corresponding to $\mathcal{G}$. Each node function depends only on the outputs of its parents, i.e., on at most $c$ variables.
\end{definition}

In essence, $f$ is built by composing a \emph{few} multi-variable subfunctions (“constituent functions”), each of which is restricted to depend on only a \emph{small} subset of variables. This property is sometimes called “hierarchical local composition” \cite{poggio2017and}.

\begin{theorem}[\citealt{MhaskarPoggio2016b}, informal]
\label{th:deepVsShallow}
Suppose $f$ is a \emph{compositionally sparse} function of dimension $d$. Then:
\begin{enumerate}
    \item \textbf{Shallow Networks:} Approximating $f$ with a one-hidden-layer network still typically requires $\Omega(\epsilon^{-d})$ parameters to achieve accuracy $\epsilon$.
    \item \textbf{Deep Networks:} A deep network that mimics the DAG structure of $f$ can achieve the same approximation accuracy with polynomial complexity in $d$ and $\epsilon^{-1}$.
\end{enumerate}
Hence, under compositional sparsity, deep architectures \emph{avoid} the curse of dimensionality.
\end{theorem}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.34\textwidth]{Figures/FunctionGraph.pdf}
    \vspace{-0.2cm}
    \caption{\emph{An illustration of a function graph $\mathcal{G}$ in the form of a DAG. The red dots represent constituent functions, each depending on a few variables (the black dots). The final output is blue. A deep network that parallels this DAG can circumvent exponential complexity.}}
    \label{DAG}
\end{figure}

\subsection{Efficient Turing-Computable Functions are Compositionally Sparse}
A highlight of \citet{poggiofraser2024} is the claim that the class of sparse compositional functions is not too small. In fact, \emph{all} efficiently Turing-computable functions on $\{0,1\}^d$ (or on real inputs with a computable representation) can be expressed by such a DAG (\cref{th:mainTuringSparse}). Below, we expand upon that argument.

\begin{definition}[Turing Computability, cf.~\citealt{sipser1996intro}]
A Boolean function $f\colon \{0,1\}^d \to \{0,1\}^k$ (or, more generally, a real-valued function $f\colon \mathbb{R}^d \to \mathbb{R}^k$) is \emph{efficiently Turing-computable} if there is a deterministic Turing machine that computes $f(x)$ in time polynomial in $d$ (and in the precision required, for the real case).
\end{definition}

\begin{theorem}[\textbf{Efficient Computability} $\implies$ \textbf{Compositional Sparsity} \cite{poggiofraser2024}]
\label{th:mainTuringSparse}
Let $f$ be a Boolean or real-valued function that is computable by a Turing machine in polynomial time. Then there exists a \emph{polynomial-size} directed acyclic graph $\mathcal{G}$ with \emph{bounded fan-in}, such that $f$ factors through the node functions of $\mathcal{G}$. In other words, $f$ is compositionally sparse.
\end{theorem}

\noindent
The proof is essentially a translation of Turing machine computation into a Boolean (or arithmetic) circuit of polynomial size and bounded fan-in. We outline the core steps below, emphasizing how they lead to the notion of compositional sparsity.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{A More Detailed Proof Sketch}
\label{sec:proofsketch}

\paragraph{Step 1: Boolean Circuits Simulate Polynomial-Time Turing Machines.}
It is well known in complexity theory that any function in the class $\mathcal{P}$ (i.e., any polynomial-time computable Boolean function) can be computed by a Boolean circuit of \emph{polynomial size} \cite{sipser1996intro}. Concretely, each step of the Turing machine is encoded by a layer of gates. After $p(d)$ steps (where $p$ is a polynomial), the final layer outputs $f(x)$.

\paragraph{Step 2: Bounded Fan-In via Circuit Transformations.}
To claim \emph{compositional sparsity}, we require that each gate in the circuit depends on only a \emph{small} number of inputs (“bounded fan-in,” typically 2). If the polynomial-size circuit has gates with large fan-in, we can systematically replace any gate of fan-in $k$ by $\mathcal{O}(\log k)$ gates of fan-in 2 arranged in a binary tree. This keeps the circuit size polynomial in $d$, but ensures \emph{each node has a small in-degree}.

\paragraph{Step 3: Composition of Sparse Gates.}
Once we have a bounded-fan-in circuit, we can view each gate as a small function of a few inputs. Stacking or layering these gates yields a composition of these small functions. This is precisely a \emph{compositionally sparse} DAG representation as in \cref{def:sparse_comp}.

\paragraph{Step 4: Real-Valued Case.}
An analogous argument applies to real-valued functions, allowing polynomial-time arithmetic circuits (with floating-point or rational representation). The \emph{fan-in bounding} step is similar: each gate with many inputs is replaced by a circuit of $O(\log k)$ bounded-fan-in gates. Thus, any real function computable in polynomial time also admits a compositionally sparse DAG.

\paragraph{Conclusion.}
Thus, every efficiently computable function is \emph{compositionally sparse}. This is exactly the missing piece to justify the universality of compositional sparsity proposed in \citet{poggiofraser2024}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Approximation by Deep Networks with Polynomially Many Weights}
\label{sec:approximation}
Given a compositionally sparse DAG of polynomial size, how do we show that a \emph{deep network} with polynomially many parameters can approximate it? The short answer is:
\begin{itemize}
    \item Each small node function (bounded fan-in) can be approximated by a \emph{small} sub-network, e.g., a one- or two-layer ridge function with parameters that scale with $\epsilon^{-c}$, where $c$ is the number of inputs to that gate.
    \item Composing these small sub-networks in the same DAG arrangement yields an overall deep network of polynomial size (since the DAG has polynomially many nodes), thereby avoiding exponential blowup in dimension $d$.
\end{itemize}
Thus, \emph{for every efficiently computable function}, there is a deep network with $\mathrm{poly}(d)$ parameters that can approximate it arbitrarily closely. This result underpins the \textbf{universality of deep, sparse networks} claimed in \citet{poggiofraser2024}.

\begin{corollary}[cf.~\citet{poggiofraser2024}]
Any efficiently computable function (Boolean or real) can be approximated up to $\epsilon$ with a deep, sparse network whose size grows polynomially in $d$ and in $\epsilon^{-1}$. Therefore, \emph{deep networks are universal approximators for all practically computable functions, without incurring the curse of dimensionality.}
\end{corollary}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Learnability and Optimization of Sparse Compositional Functions}
\label{sec:learnability}
Although compositional sparsity shows \emph{why} deep networks can represent efficiently computable functions with polynomial capacity, it does not \emph{automatically} ensure that we can \emph{find} that representation from finite data using standard training algorithms. This section discusses the more conjectural problem of \emph{learnability}.

\subsection{Challenges of Learning Compositional Functions}
\label{sec:challenges}
In principle, a function $f$ might factor as $g_L \circ g_{L-1} \circ \dots \circ g_1$ with each $g_i$ depending on only a few variables. But from purely \emph{input-output} data $(x, f(x))$, it may be challenging for gradient descent to discover each $g_i$. Indeed:
\begin{itemize}
    \item The search space is highly non-convex, so local minima abound.
    \item Some subfunctions might be “hidden” behind subsequent compositions.
    \item We do \emph{not} always know the DAG architecture \emph{a priori}.
\end{itemize}
These issues reflect fundamental complexity-theoretic challenges. Learning general Boolean circuits from input-output examples is known to be NP-hard (under standard complexity assumptions) in the worst case \cite{BlumRivest1989}. The question then is: \emph{How might deep learning circumvent or mitigate such hardness?}

\subsection{Recent Trends Suggesting Hierarchical Training}
\label{sec:recenttrends}
Despite the potential hardness, modern empirical successes in deep learning appear to leverage certain forms of \emph{hierarchical} or \emph{modular} training:
\begin{enumerate}
    \item \textbf{Chain-of-Thought (CoT) Prompting} \cite{Wei2022chainofthought}:
    Instead of purely input-output pairs, the model is trained or prompted with intermediate “thinking steps.” This effectively \emph{exposes} partial subfunctions $g_i$. 
    \item \textbf{Tree-of-Thought or Hierarchical Decomposition} \cite{yao2023tree,bubeck2023unreasonable}:
    More elaborate prompting strategies break a problem into subproblems (akin to subfunctions).
    \item \textbf{Layer-wise or Block-wise Pretraining}:
    Some large neural architectures (including certain language models) receive partial supervision or constraints on intermediate layers.
    \item \textbf{Specialized Architectures (CNNs, Transformers)}:
    Convolutional neural networks \emph{impose} local structure that mimics a compositional DAG, especially in images. Transformers with self-attention can similarly \emph{focus} on a small subset of tokens, providing a path to learned sparsity \cite{han2023transformers,poggio2017and}.
\end{enumerate}

All these strategies can be viewed as partial or explicit \emph{revelation of compositional structure}, thereby simplifying the learning or optimization.

\subsection{Conjecture}
\begin{quote}
\emph{Useful learning algorithms exploit compositionality by either (i) imposing a known DAG structure (as in CNNs) or (ii) providing direct or indirect supervision for subfunctions (as in chain-of-thought or other hierarchical training).}
\end{quote}
This is consistent with the principle that, in the general worst case, we do not expect an efficient learning procedure for arbitrary circuits \cite{BlumRivest1989}. But real-world tasks are rarely adversarial. Instead, they appear to be governed by compositional structure that can be revealed through architecture or additional training signals.

\subsection{New Sub-Subsection: Symmetry Groups in Compositional DAGs}
\label{sec:symmetry-subsection}

\noindent
\textbf{**ADDED TEXT**:}

\noindent
\textbf{7.1 Symmetry Groups and Training Degeneracies}

\noindent
\textbf{\boldmath
In addition to these learnability considerations, compositional DAGs often exhibit fewer symmetries or automorphisms compared to dense, fully-connected architectures \cite{Bondy2008,Cormen2009}. Dense networks can have large symmetry groups because permuting neurons or layers may leave the overall function invariant \cite{Brea2019,Kawaguchi2016}. Such symmetries can create a highly degenerate loss landscape, requiring many training steps to “break” them in practice \cite{Kawaguchi2016, Brea2019}. By contrast, in compositionally sparse DAGs, each subfunction is restricted in terms of which inputs or parent nodes it can depend on, thus reducing the automorphism group to a much smaller set of permutations \cite{Mhaskar2017,Poggio2017}. This means that if we already know or can partially discover the DAG structure, we may face fewer degenerate optima and a potentially simpler optimization landscape \cite{LeCun2015}.
}

\textit{\small (Why it makes sense here: The professor’s notes emphasize how “fewer symmetries” in a DAG can reduce degeneracies during training. Since Section 7 is about learnability and optimization challenges, this new paragraph naturally extends that discussion by connecting the symmetry argument to the difficulty/ease of gradient descent. Each sentence includes a citation, matching the requirement that no new statement stands without a reference.)}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Alternative Views}
\label{sec:alternativeviews}
While we argue that \emph{compositional sparsity} is central, other researchers emphasize different principles for explaining deep learning’s successes. For instance, an alternative viewpoint is that network \emph{overparameterization} and implicit regularization alone drive generalization and efficiency; in this view, compositional structure plays a lesser role. Another perspective highlights \emph{lottery tickets} or \emph{emergent modularity} in large networks, suggesting that discovered substructures need not be strictly compositional from the outset.

We acknowledge these viable perspectives. Nevertheless, our position remains that imposing (or learning) explicit compositional hierarchies is a powerful and parsimonious way to bypass the curse of dimensionality and make efficient training more tractable. Indeed, these alternative views might be complementary, offering additional explanations for how compositional patterns emerge or are exploited in practice.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Discussion and Outlook}
\label{sec:discussion}
We have surveyed a core theoretical insight: \emph{efficient Turing computability coincides with compositional sparsity, and deep networks can approximate compositionally sparse functions with polynomial capacity.} This explains \emph{why} deep neural networks might be powerful enough to model any practically computable function without incurring the curse of dimensionality.

Yet there is a crucial gap from \emph{approximation} to \emph{learning.} While universal approximation guarantees expressiveness, the question of how to \emph{train} such networks effectively from finite data remains nontrivial, especially in the absence of explicit compositional constraints. Empirically, modern methods from Convolutional Neural Networks to Transformers with chain-of-thought prompting appear to exploit compositionality, either by design or through the data.

\subsection{Implications for Theorem Proving and Mathematics}
As noted by \citet{poggiofraser2024}, compositional sparsity may be relevant to mathematics at large, including automated theorem proving. If mathematical statements can be decomposed into a series of short steps (each depending on only a few prior facts), then a deep network may, in principle, replicate that proof search. A direct \emph{theorem-proving} application, however, raises the same key question: \emph{Is the compositional structure fully exposed to the network?} If the sub-lemmas or intermediate steps are hidden, learning might become intractable. Providing partial supervision in the form of previously discovered lemmas or hints could mitigate the complexity. This resonates with the hierarchical nature of human-driven proofs \cite{gowers2022auto}.

\subsection{Conclusions}
Compositional sparsity represents a unifying principle:
\begin{itemize}
    \item \emph{Approximation:} Explains how deep networks can approximate complex tasks—indeed, all polynomial-time computable functions—without exponential blowup.
    \item \emph{Generalization:} Enables smaller effective dimensionality, thereby mitigating overfitting in practice.
    \item \emph{Learnability:} Suggests that discovering or revealing the “right” compositional DAG is the main difficulty, tackled in part by specialized architectures (CNNs) or new training paradigms (CoT, hierarchical prompting).
\end{itemize}

Future work in deep learning theory will likely refine these ideas further, especially around the open-ended \emph{optimization} challenge of how best to discover compositional structure from limited supervision. If we can more directly incorporate compositional assumptions (e.g., by guiding subfunction learning), we may see further breakthroughs in efficiency and interpretability.

% \subsection*{Acknowledgements}
% This work was partially supported by the Center for Brains, Minds, and Machines (CBMM) at MIT, NSF grant CCF-1231216, and other research sponsors. We thank many colleagues for discussions on these topics, especially members of the CBMM community.

\bibliographystyle{icml2025}
\bibliography{references}

\end{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% TODO:
% Rewrite stuff
% All sources should be correct, there are a lot of missing links etc.
% The paper needs to be compliant with the ICML requirements found here: https://icml.cc/Conferences/2025/CallForPositionPapers 
% The transition between different parts can be more coherent, right now its not smooth
% Add more Authors!!!!!
% Re-add acknowledgments for the camera-ready version
% Symetries Sparse Compositionality (Rafael)
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass{article}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% ICML 2025 STYLE SETUP
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% TEST
\usepackage{enumitem}

% hyperref makes hyperlinks in the resulting PDF.
\usepackage{hyperref}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the camera-ready submission:
% \usepackage[accepted]{icml2025}
\usepackage{icml2025}

% For theorems and such
\usepackage{amsmath,amssymb,amsthm}
\usepackage{mathtools}
\usepackage{todonotes}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}

\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}

\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Title and Author Setup
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\icmltitlerunning{Position: A Theory of Deep Learning must include Compositional Sparsity}

\begin{document}

\twocolumn[
\icmltitle{Position: A Theory of Deep Learning must include Compositional Sparsity}

\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Davide D'Ascenzo}{equal,mit,torino}
\icmlauthor{Rafael Dubach}{equal,mit,uzh}
\icmlauthor{Tomaso Poggio}{equal,mit}
\end{icmlauthorlist}

\icmlaffiliation{mit}{Center for Brains, Minds and Machines (CBMM), MIT, Cambridge, MA, USA}
\icmlaffiliation{uzh}{University of Zurich, Zurich, Switzerland}
\icmlaffiliation{torino}{Politecnico di Torino, Torino, Italy}

\icmlcorrespondingauthor{Davide D'Ascenzo}{kidara@mit.edu}
\icmlcorrespondingauthor{Rafael Dubach}{raduba@mit.edu}
\icmlcorrespondingauthor{Tomaso Poggio}{tp@mit.edu}

\vskip 0.3in
]

\printAffiliationsAndNotice{}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% ABSTRACT
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{abstract}  
Neural networks have demonstrated impressive success in various domains, raising the question of what fundamental principles underlie the effectiveness of the best AI systems and quite possibly of human intelligence. \textbf{This position paper} argues that \emph{compositional sparsity}, the property that a compositional function has “few” constituent functions, each depending on only a small subset of inputs, is a key principle underlying successful learning architectures. Surprisingly, \emph{all} functions that are efficiently Turing-computable admit a compositionally sparse representation. This implies that deep and sparse neural networks are a good choice to learn from examples in a classical ERM framework, since they are a parametric representation that requires at most a polynomial number of parameters for a large class of functions (class $P$ in the Boolean case). The property of compositional sparsity explains why depth and sparsity are important in learning, why neural networks do not suffer the curse of dimensionality and why convolutional networks are better than dense networks.  Finally,  intriguing \emph{learnability} questions remain, suggesting that other properties, complementing compositional sparsity, need to be characterized for a more satisfying learning theory.  
\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% INTRODUCTION
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}
The current era is marked by the emergence of powerful deep learning architectures that are transforming industries and society. Yet, we still lack a fundamental understanding of why deep networks work and what their limitations are. It was a similar situation with electricity after Volta invention of the \emph{pila} opened the way to the telegraph, the electric motor and electric generators despite the lack of a theory, which was finally achieved by Maxwell in 1864.

In the field of deep learning we are between Volta and Maxwell: we still need to achieve a fundamental theoretical understanding of deep learning, its architectures and algorithms. There are many reasons why this is important. Present attempts to understand, predict and control behaviors of machine learning models are not based on fundamental principles and are unlikely to be successful\todo{Is this claim too strong?}. On the other hand, there is a real hope that such fundamental principles may exist and can be discovered as shown by recent progress in the theory of deep learning.
Such hope is indeed motivated by the impressive achievements of deep learning in various domains. AlphaFold has revolutionized protein structure prediction \cite{alphafold_1,alphafold_2}, large language models like GPT-3 have demonstrated impressive natural language generation capabilities \cite{gpt3}, AlphaGeometry has been able to solve International Mathematical Olympiad problems \cite{alphageometry}.

These successes have sparked intense interest in understanding the theoretical principles underlying such versatile capabilities. Why do neural networks, in particular, \emph{deep} architectures, often scale so effectively to high-dimensional tasks?

Historically, attempts to build intelligent machines \cite{McCarthy_1955} often emphasized \emph{rule-based} or \emph{expert systems}. In contrast, modern breakthroughs primarily stem from learning architectures, especially multi-layered neural networks. This shift has prompted renewed interest in learning theory foundations, focusing on three core questions:
\begin{enumerate}
    \item \textbf{Approximation capacity:} How well can neural networks approximate functions of interest?
    \item \textbf{Optimization:} How can we train (i.e., optimize) network parameters on finite training data?
    \item \textbf{Generalization:} Why do networks not overfit, or at least, how do they still perform well despite their large capacity?
\end{enumerate}

\noindent
\textbf{Our position is that compositional sparsity is a key principle in explaining why deep networks can scale to high-dimensional tasks efficiently}. In this paper, we focus primarily on approximation and optimization but also highlight connections to generalization. We argue, building on \citet{poggiofraser2024}, that compositional sparsity:

\begin{itemize}
    \item Explains how deep networks can approximate a very large class of functions (namely, all efficiently Turing-computable functions) \emph{without} requiring an exponential number of parameters;
    \item Suggests that discovering and training such representations can be challenging, yet potentially more tractable when the compositional structure is partially revealed (e.g., via chain-of-thought, or by imposing architectural constraints).
\end{itemize}

The paper is structured as follows. In \cref{sec:preliminaries}, we provide a brief overview of learning theory and the importance of the hypothesis class $\mathcal{H}$. In \cref{sec:shallow-deep}, we discuss the limitations of shallow networks and the advantages of deep architectures. In \cref{sec:compositional_sparsity}, we introduce the concept of compositional sparsity and its implications for deep learning. In \cref{sec:proofsketch}, we provide a more detailed proof sketch of why efficiently computable functions are compositionally sparse. In \cref{sec:learnability}, we discuss the challenges of learning compositional functions and conjectural perspectives on how to address them. Finally, in \cref{sec:alternativeviews}, we discuss alternative viewpoints and conclude with a discussion and outlook in \cref{sec:discussion}.

% Below, we first expand on results from \citet{poggiofraser2024}, providing additional detail on the proof that \emph{efficiently Turing-computable functions are compositionally sparse and can thus be approximated by deep networks with polynomially many weights.} We then discuss the second crucial question: \emph{How do we learn or optimize such representations in practice?} We present conjectural perspectives on why chain-of-thought or other hierarchical training approaches might be essential.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% AMS RESULTS: COMPOSITIONAL SPARSITY OF EFFICIENTLY COMPUTABLE FUNCTIONS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Preliminaries in Learning Theory}
\label{sec:preliminaries}
We begin by briefly reviewing the essential setup of supervised learning, as in \citet{vapnik2013nature,hastie2009elements,james2023introduction}.

A supervised learning problem is defined by an unknown probability distribution $\mu$ on $\mathcal{X} \times \mathcal{Y}$, from which one draws an i.i.d.~training set $S = \{(x_i, y_i)\}_{i=1}^m$. If one views $y_i$ as a (possibly noisy) label associated with input $x_i$, the underlying goal is to approximate the \emph{target function} $f_\mu \colon \mathcal{X} \to \mathcal{Y}$ that best explains the data in some sense, typically measured by the expected loss
\[
\mathcal{L}(f) \;=\; \int (f(x) - y)^2 \, d\mu(x,y)
\]
(for the squared error loss, for instance). Since $\mu$ is unknown, one instead performs \emph{empirical risk minimization} over a chosen \emph{hypothesis class} $\mathcal{H}$:
\[
f_S \;=\; \arg\min_{f \in \mathcal{H}} \Bigl\{\tfrac{1}{m}\!\sum_{i=1}^m (f(x_i) - y_i)^2 \;+\; \text{regularizer}(f)\Bigr\},
\]
aiming to ensure that $f_S$ also performs well on unseen data (generalization).  

From an approximation-theoretic perspective, the choice of $\mathcal{H}$ is crucial. If $\mathcal{H}$ is too narrow, it may not approximate $f_\mu$ well; if it is too large, searching it might be intractable or lead to poor generalization. \emph{Deep neural networks} (DNNs) have emerged as a surprisingly effective balance between expressiveness and search, but a core question remains: \emph{Which property of real-world target functions, $f_\mu$, ensures that a suitable deep network in $\mathcal{H}$ can approximate it with polynomially many parameters?}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Shallow vs. Deep Approximation and the Curse of Dimensionality}
\label{sec:shallow-deep}
Classical approximation theory has shown that \emph{shallow} networks (i.e., one-hidden-layer networks) can approximate any continuous function arbitrarily well on a compact set \cite{Pinkus_1999}. However, the number of parameters can grow \emph{exponentially} in the input dimension $d$. This well-known phenomenon is referred to as the \emph{curse of dimensionality} \cite{Bellman1957}.

\paragraph{Shallow Approximation Bound.}
The limitations of shallow networks become apparent when considering the approximation of functions in Sobolev or H\"older classes, denoted as $\mathrm{W}_{s}^d$. For such function classes, a shallow network can achieve an $\epsilon$-approximation (in uniform or $L^2$ norm) only if its size $N$ satisfies roughly $N = \mathcal{O}(\epsilon^{-d/s})$, which becomes infeasible as $d$ grows \cite{Pinkus_1999}. The smoothness parameter $s$ partially mitigates this effect, but the curse of dimensionality persists.

\paragraph{When is Deep Better?}
A growing body of work \cite{MhaskarPoggio2016b,dahmen_compositional_2023, cagnetta_compositional_2024, poggiofraser2024} has demonstrated that if the target function $f(x_1, \dots, x_d)$ exhibits a \emph{compositional} structure, then a \emph{deep} network that mirrors this structure can break the exponential dependence on $d$. This compositional property allows the function $f$ to be decomposed into a hierarchy of subfunctions, each depending on only a small subset of variables. These subfunctions are then composed in a nested manner, creating a multi-level structure. Such an arrangement enables deep networks to exploit this inherent hierarchy, leading to more efficient function approximation. This efficiency stems from the network's ability to learn and represent these subfunctions at different levels of abstraction, effectively breaking down the complex target function into more manageable components. Consequently, deep architectures can achieve superior approximation capabilities compared to their shallow counterparts for a wide class of practically relevant functions, particularly in high-dimensional spaces.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Compositional Sparsity}
\label{sec:compositional_sparsity}

\subsection{Definition and Main Theorem}
\begin{definition}[Sparse Compositional Function]
\label{def:sparse_comp}
A function $f:\mathbb{R}^d \rightarrow \mathbb{R}^k$ is \emph{compositionally sparse} if there exists a directed acyclic graph (DAG) $\mathcal{G}$ of polynomial size in $d$, whose internal nodes each have in-degree at most $c$ (a small constant), such that $f$ factors as a composition of node functions corresponding to $\mathcal{G}$. Each node function depends only on the outputs of its parents, i.e., on at most $c$ variables.
\end{definition}

In essence, $f$ is built by composing a \emph{few} multi-variable subfunctions (``constituent functions''), each of which is restricted to depend on only a \emph{small} subset of variables. This property is sometimes called ``hierarchical local composition'' \cite{poggio2017and}.


\begin{theorem}[\citealt{MhaskarPoggio2016b}, informal]
\label{th:deepVsShallow}
Suppose $f$ is a \emph{compositionally sparse} function of dimension $d$. Then:
\begin{enumerate}
    \item \textbf{Shallow Networks:} Approximating $f$ with a one-hidden-layer network still typically requires $\Omega(\epsilon^{-d})$ parameters to achieve accuracy $\epsilon$.
    \item \textbf{Deep Networks:} A deep network that mimics the DAG structure of $f$ can achieve the same approximation accuracy with polynomial complexity in $d$ and $\epsilon^{-1}$. 
\end{enumerate}
Hence, under compositional sparsity, deep architectures \emph{avoid} the curse of dimensionality.
\end{theorem}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.34\textwidth]{Figures/FunctionGraph.pdf}
    \vspace{-0.2cm}
    \caption{\emph{An illustration of a function graph $\mathcal{G}$ in the form of a DAG. The red dots represent constituent functions, each depending on a few variables (the black dots). The final output is blue. A deep network that parallels this DAG can circumvent exponential complexity.}}
    \label{DAG}
\end{figure}

\subsection{Efficient Turing-Computable Functions are Compositionally Sparse}
A highlight of \citet{poggiofraser2024} is the claim that the class of sparse compositional functions is not too small. In fact, \emph{all} efficiently Turing-computable functions on $\{0,1\}^d$ (or on real inputs with a computable representation) can be expressed by such a DAG (\cref{th:mainTuringSparse}). Below, we expand upon that argument.

\begin{definition}[Turing Computability, cf.~\citealt{sipser1996intro}]
A Boolean function $f\colon \{0,1\}^d \to \{0,1\}^k$ (or, more generally, a real-valued function $f\colon \mathbb{R}^d \to \mathbb{R}^k$) is \emph{efficiently Turing-computable} if there is a deterministic Turing machine that computes $f(x)$ in time polynomial in $d$ (and in the precision required, for the real case).
\end{definition}

\begin{theorem}[\textbf{Efficient Computability} $\implies$ \textbf{Compositional Sparsity} \cite{poggiofraser2024}]
\label{th:mainTuringSparse}
Let $f$ be a Boolean or real-valued function that is computable by a Turing machine in polynomial time. Then there exists a \emph{polynomial-size} directed acyclic graph $\mathcal{G}$ with \emph{bounded fan-in}, such that $f$ factors through the node functions of $\mathcal{G}$. In other words, $f$ is compositionally sparse.
\end{theorem}

\noindent
The proof is essentially a translation of Turing machine computation into a Boolean (or arithmetic) circuit of polynomial size and bounded fan-in. We outline the core steps below, emphasizing how they lead to the notion of compositional sparsity.

\section{A More Detailed Proof Sketch}
\label{sec:proofsketch}

\paragraph{Step 1: Boolean Circuits Simulate Polynomial-Time Turing Machines.}
It is well known in complexity theory that any function in the class $\mathcal{P}$ (i.e., any polynomial-time computable Boolean function) can be computed by a Boolean circuit of \emph{polynomial size} \cite{sipser1996intro}. Concretely, each step of the Turing machine is encoded by a layer of gates. After $p(d)$ steps (where $p$ is a polynomial), the final layer outputs $f(x)$.

\paragraph{Step 2: Bounded Fan-In via Circuit Transformations.}
To claim \emph{compositional sparsity}, we require that each gate in the circuit depends on only a \emph{small} number of inputs (``bounded fan-in,'' typically 2). If the polynomial-size circuit has gates with large fan-in, we can systematically replace any gate of fan-in $k$ by $\mathcal{O}(\log k)$ gates of fan-in 2 arranged in a binary tree. This keeps the circuit size polynomial in $d$, but ensures \emph{each node has a small in-degree}.

\paragraph{Step 3: Composition of Sparse Gates.}
Once we have a bounded-fan-in circuit, we can view each gate as a small function of a few inputs. Stacking or layering these gates yields a composition of these small functions. This is precisely a \emph{compositionally sparse} DAG representation as in \cref{def:sparse_comp}.

\paragraph{Step 4: Real-Valued Case.}
An analogous argument applies to real-valued functions, allowing polynomial-time arithmetic circuits (with floating-point or rational representation). The \emph{fan-in bounding} step is similar: each gate with many inputs is replaced by a circuit of $O(\log k)$ bounded-fan-in gates. Thus, any real function computable in polynomial time also admits a compositionally sparse DAG.

\paragraph{Conclusion.}
Thus, every efficiently computable function is \emph{compositionally sparse}. This is exactly the missing piece to justify the universality of compositional sparsity proposed in \citet{poggiofraser2024}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPROXIMATION BY DEEP NETWORKS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Approximation by Deep Networks with Polynomially Many Weights}
Given a compositionally sparse DAG of polynomial size, how do we show that a \emph{deep network} with polynomially many parameters can approximate it? The short answer is:
\begin{itemize}
    \item Each small node function (bounded fan-in) can be approximated by a \emph{small} sub-network, e.g., a one- or two-layer ridge function with parameters that scale with $\epsilon^{-c}$, where $c$ is the number of inputs to that gate.
    \item Composing these small sub-networks in the same DAG arrangement yields an overall deep network of polynomial size (since the DAG has polynomially many nodes), thereby avoiding exponential blowup in dimension $d$.
\end{itemize}
Thus, \emph{for every efficiently computable function}, there is a deep network with $\mathrm{poly}(d)$ parameters that can approximate it arbitrarily closely. This result underpins the \textbf{universality of deep, sparse networks} claimed in \citet{poggiofraser2024}.

\begin{corollary}[cf.~\citet{poggiofraser2024}]
Any efficiently computable function (Boolean or real) can be approximated up to $\epsilon$ with a deep, sparse network whose size grows polynomially in $d$ and in $\epsilon^{-1}$. Therefore, \emph{deep networks are universal approximators for all practically computable functions, without incurring the curse of dimensionality.}
\end{corollary}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% LEARNABILITY & OPTIMIZATION
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Learnability and Optimization of Sparse Compositional Functions}
\label{sec:learnability}
Although compositional sparsity shows \emph{why} deep networks can represent efficiently computable functions with polynomial capacity, it does not \emph{automatically} ensure that we can \emph{find} that representation from finite data using standard training algorithms. This section discusses the more conjectural problem of \emph{learnability}.

\subsection{Challenges of Learning Compositional Functions}
\label{sec:challenges}
In principle, a function $f$ might factor as $g_L \circ g_{L-1} \circ \dots \circ g_1$ with each $g_i$ depending on only a few variables. But from purely \emph{input-output} data $(x, f(x))$, it may be challenging for gradient descent to discover each $g_i$. Indeed:
\begin{itemize}
    \item The search space is highly non-convex, so local minima abound.
    \item Some subfunctions might be “hidden” behind subsequent compositions.
    \item We do \emph{not} always know the DAG architecture \emph{a priori}.
\end{itemize}
These issues reflect fundamental complexity-theoretic challenges. Learning general Boolean circuits from input-output examples is known to be NP-hard (under standard complexity assumptions) in the worst case \cite{BlumRivest1989}. The question then is: \emph{How might deep learning circumvent or mitigate such hardness?}


\subsection{Recent Trends Suggesting Hierarchical Training}
\label{sec:recenttrends}
Despite the potential hardness, modern empirical successes in deep learning appear to leverage certain forms of \emph{hierarchical} or \emph{modular} training:
\begin{enumerate}
    \item \textbf{Chain-of-Thought (CoT) Prompting} \cite{Wei2022chainofthought}:
    Instead of purely input-output pairs, the model is trained or prompted with intermediate “thinking steps.” This effectively \emph{exposes} partial subfunctions $g_i$. 
    \item \textbf{Tree-of-Thought or Hierarchical Decomposition} \cite{yao2023tree,bubeck2023unreasonable}:
    More elaborate prompting strategies break a problem into subproblems (akin to subfunctions).
    \item \textbf{Layer-wise or Block-wise Pretraining}:
    Some large neural architectures (including certain language models) receive partial supervision or constraints on intermediate layers. 
    \item \textbf{Specialized Architectures (CNNs, Transformers)}:
    Convolutional neural networks \emph{impose} local structure that mimics a compositional DAG, especially in images. Transformers with self-attention can similarly \emph{focus} on a small subset of tokens, providing a path to learned sparsity \cite{poggio2017and,han2023transformers}.
\end{enumerate}

All these strategies can be viewed as partial or explicit \emph{revelation of compositional structure}, thereby simplifying the learning or optimization.

\subsection{Conjecture}
\begin{quote}
\emph{Useful learning algorithms exploit compositionality by either (i) imposing a known DAG structure (as in CNNs) or (ii) providing direct or indirect supervision for subfunctions (as in chain-of-thought or other hierarchical training).}
\end{quote}
This is consistent with the principle that, in the general worst case, we do not expect an efficient learning procedure for arbitrary circuits \cite{BlumRivest1989}. But real-world tasks are rarely adversarial. Instead, they appear to be governed by compositional structure that can be revealed through architecture or additional training signals.

%% Tommys email 01/19/25
\subsection{Symmetry Groups in Compositional DAGs}
\label{sec:symmetry-subsection}

%\textbf{7.1 Symmetry Groups and Training Degeneracies}

\noindent
\textit{In addition to these learnability considerations, compositional DAGs often exhibit fewer symmetries or automorphisms compared to dense, fully-connected architectures \cite{Bondy2008,Cormen2009}. Dense networks can have large symmetry groups because permuting neurons or layers may leave the overall function invariant \cite{Brea2019,Kawaguchi2016}. Such symmetries can create a highly degenerate loss landscape, requiring many training steps to “break” them in practice \cite{Kawaguchi2016, Brea2019}. By contrast, in compositionally sparse DAGs, each subfunction is restricted in terms of which inputs or parent nodes it can depend on, thus reducing the automorphism group to a much smaller set of permutations \cite{Mhaskar2017,Poggio2017}. This means that if we already know or can partially discover the DAG structure, we may face fewer degenerate optima and a potentially simpler optimization landscape \cite{LeCun2015}.
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% ALTERNATIVE VIEWS (REQUIRED SECTION)
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Alternative Views}
\label{sec:alternativeviews}
While we argue that \emph{compositional sparsity} is central, other researchers emphasize different principles for explaining deep learning’s successes. For instance, an alternative viewpoint is that network \emph{overparameterization} and implicit regularization alone drive generalization and efficiency; in this view, compositional structure plays a lesser role. Another perspective highlights \emph{lottery tickets} or \emph{emergent modularity} in large networks, suggesting that discovered substructures need not be strictly compositional from the outset.  

We acknowledge these viable perspectives. Nevertheless, our position remains that imposing (or learning) explicit compositional hierarchies is a powerful and parsimonious way to bypass the curse of dimensionality and make efficient training more tractable. Indeed, these alternative views might be complementary, offering additional explanations for how compositional patterns emerge or are exploited in practice.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% DISCUSSION AND OUTLOOK
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Discussion and Outlook}
\label{sec:discussion}
We have surveyed a core theoretical insight: \emph{efficient Turing computability coincides with compositional sparsity, and deep networks can approximate compositionally sparse functions with polynomial capacity.} This explains \emph{why} deep neural networks might be powerful enough to model any practically computable function without incurring the curse of dimensionality.

Yet there is a crucial gap from \emph{approximation} to \emph{learning.} While universal approximation guarantees expressiveness, the question of how to \emph{train} such networks effectively from finite data remains nontrivial, especially in the absence of explicit compositional constraints. Empirically, modern methods from Convolutional Neural Networks to Transformers with chain-of-thought prompting, appear to exploit compositionality, either by design or through the data.



\subsection{Implications for Theorem Proving and Mathematics}
As noted by \citet{poggiofraser2024}, compositional sparsity may be relevant to mathematics at large, including automated theorem proving. If mathematical statements can be decomposed into a series of short steps (each depending on only a few prior facts), then a deep network may, in principle, replicate that proof search. A direct \emph{theorem-proving} application, however, raises the same key question: \emph{Is the compositional structure fully exposed to the network?} If the sub-lemmas or intermediate steps are hidden, learning might become intractable. Providing partial supervision in the form of previously discovered lemmas or hints could mitigate the complexity. This resonates with the hierarchical nature of human-driven proofs \cite{gowers2022auto}.

\subsection{Conclusions}
Compositional sparsity represents a unifying principle:
\begin{itemize}
    \item \emph{Approximation:} Explains how deep networks can approximate complex tasks indeed, all polynomial-time computable functions without exponential blowup.
    \item \emph{Generalization:} Enables smaller effective dimensionality, thereby mitigating overfitting in practice.
    \item \emph{Learnability:} Suggests that discovering or revealing the “right” compositional DAG is the main difficulty, tackled in part by specialized architectures (CNNs) or new training paradigms (CoT, hierarchical prompting).
\end{itemize}

Future work in deep learning theory will likely refine these ideas further, especially around the open-ended \emph{optimization} challenge of how best to discover compositional structure from limited supervision. If we can more directly incorporate compositional assumptions (e.g., by guiding subfunction learning), we may see further breakthroughs in efficiency and interpretability.

% \subsection*{Acknowledgements}
% This work was partially supported by the Center for Brains, Minds, and Machines (CBMM) at MIT, NSF grant CCF-1231216, and other research sponsors. We thank many colleagues for discussions on these topics, especially members of the CBMM community.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% REFERENCES
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bibliographystyle{icml2025}
\bibliography{references}
\end{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Starting Paper Tommy
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \begin{abstract}
% Neural networks have demonstrated impressive success in various domains, raising the question of what fundamental principles underlie the effectiveness of the best AI systems and quite possibly of human
% intelligence. The claim here is that compositional sparsity, or the property that only a small subset of inputs contribute to each of the constituent functions of a compositional function, is the key principle underlying successful learning architectures. Surprisingly, compositional sparsity turns out to hold for most functions of interest, because it holds for all functions that are Turing computable.
% This is the reason why deep networks are the parametric estimator in the ERM framework of machine learing. Sparse constituent functions however are not always easy to learn from the optimization point of view unless they are very sparse or have a specific structure. We argue that the current trends in training transformers -- such as CoT and QSTaR -- follow the logic of providing data such to support the learning of sparse constituent functions that are easy to learn.
% \end{abstract}
% \section{Introduction}
% In this deep learning era we are developing powerful applications but we still do not fully understand why deep networks work and what their limitations are. It was a similar situation with electricity after Volta invention of the \emph{pila} opened the way to the telegraph, the electric motor and electric generators despite the lack of a theory, which was finally achieved by Maxwell in 1864. In the field of deep learning we are between Volta and Maxwell: we still need to achieve a fundamental theoretical understanding of deep learning, its architectures and algorithms. There are many reasons why this is important. Present attempts to understand, predict and control behaviors of machine learning models are not based on fundamental principles and are unlikely to be successful. On the other hand, there is a real hope that such fundamental principles may exist and can be discovered as shown by recent progress in the theory of deep learning. 
% \vspace{0.05in}

% The Machine Learning community has made significant progress in developing foundations for a theory of approximation and generalization of deep networks. Key missing parts of the theory are in the area of optimization -- that is on the training phase of deep networks. This paper argues about what should be done to fill this gap. 


% \section{Parametric ERM framework}

% We sketch very briefly some basic ideas of learning theory to provide some mathematical context to our  proposal (see \cite{hastie2009elements,vapnik2013nature,james2023introduction}).
% A supervised learning problem is defined by an unknown probability measure $\mu$ on the product space ${X}\times {Y}$ from which the training data $S = \{(x_i, y_i)\}^{m}_{i=1}$  is drawn. 
% The measure $\mu$ defines a function $f_{\mu}: {X} \rightarrow{Y}$. Thus $f_{\mu}$ can be viewed as 
% the true input-output function reflecting the environment which produces the data. The goal of supervised learning is to ``closely'' approximate this function. To do so, a parametric approximator $f$ is used, and closeness is measured by the expected error of $f$ which may be defined by $ I(f)= \mathbb{E}_z \ell (f, z)= \int_X \ell(f_\mu, f) d{\mu}$ where $\ell: \mathcal{H} \times Z \to [0, \infty)$ with $z=(x,y)$ is a loss function. 
% To ``find'' $f$ minimizing the error $I(f)$ for an unknown $f_\mu$, one uses a supervised learning strategy which consists of two steps: \
% \textbf{(step 1)} Choose a space $\mathcal{H}$ of {\it parametric functions $f$} where search, that is optimization, is possible.
% \textbf{(step 2)} Searching within $\mathcal{H}$, one usually minimize the {\it empirical error} (also called {\it empirical risk}) $I_S (f) = \frac{1}{m}\sum_{i=1}^m \ell \left(f, z_i \right)$ (or its regularized version) to obtain a minimizer $f_S: {X} \rightarrow {Y}$. This approach is called {\it (regularized) empirical risk minimization (ERM)}. This solution is studied through a mathematical analysis \textbf{(step 3)} that relies on appropriate decompositions of the  error. Here we are interested in the  {\it generalization error} defined to be $I_S\left(f_S \right)-I\left( f_S \right)$ -- the difference between the empirical error of our minimizer and its expected error. We are also interested in the {\it approximation error} defined as $I\left( f_{\mathcal{H}} \right) - I\left( f_0 \right )$ -- the difference between the errors of the best function in $\mathcal{H}$ and of the best possible function $f_0$. We are also interested in the {\it optimization error} which is defined as $I_S\left(f_S\right)-I_S\left(f_{\mathcal{H}}\right)$ -- the difference between the empirical error of the minimizer of the empirical risk and the empirical error of the best function in $\mathcal{H}$.


% \section{Why deep networks are a good choice for $\mathcal{H}$}
% %as deep, sparse networks.}
% \begin{figure}{r}{0.5\textwidth}
%     \centering 
% %    \vspace{-13pt}
% \includegraphics[width=.34\textwidth]{Figures/FunctionGraph.pdf}
%     \caption{\it An illustration of a function graph $\mathcal{G}$ in the form of a Directed Acyclic Graph (DAG). In this graph, the vertices are represented by red dots, while the input to the different nodes are depicted as black dots, connected by edges to the corresponding red nodes. The red dots represent the constituent functions; each has the dimensionality corresponding to the number of incoming edges. The output value of the function $f$  is represented by the blue dot. The approximating network uses a sum of ridge functions $\sum_{i=1}^r c_i \sigma(W_i  x)$ for each red node.}
%   %  \vspace{-12pt}
%     \label{DAG}
% \end{figure}

% Clearly $\mathcal{H}$ should be chosen to be capable of closely approximate a large class of target functions while remaining computationally efficient for optimization on the training data. 
% Efficiency implies that the number of parameters in the approximators must not be exponential in $d$, the number of (real- or Boolean-valued) input variables. On the other hand, it is well known that for many function classes, a number of parameters exponential in $d/s$ may be required to achieve a desired accuracy, where $s$ is a measure of smoothness such as the number of bounded derivatives. This is an example of the so-called {\it curse of dimensionality} \cite{Bellman:1957}. 
% The question is then: How can our choice of $\mathcal{H}$ avoid the curse of dimensionality?

% We first recall that {\it all functions have a compositional representation which is not unique}, since in general, a function admits more than one compositional representation. We then define the class of {\it sparse compositional functions} which are the composition of a few sparse constituent functions. Notice that the assumption of sparse target functions has often appeared in the recent approximation literature (see \cite{MhaskarPoggio2016b,dahmen2022compositional,10.1214/19-AOS1912,10.1214/19-AOS1875,bachmayr2021approximation,10.1214/19-AOS1911}). 

% % \vspace{0.05in}

% \begin{definition}
% \label{Definition23}
% {\it Consider a function $f:\mathbb{R}^{d'} \to \mathbb{R}$ that depends on at most $d_0 \leq d'$ variables. When $d_0$ is such that there is no curse of dimensionality for $f$ -- then we will say that $f$ is ``sparse". Furthermore, a function $g:\mathbb{R}^{d} \to \mathbb{R}$ is a compositionally sparse function if it can be represented as the composition of no more than $poly(d)$ sparse functions.
% } 
% \end{definition}


% In the following, for simplicity of notation, we assume that the smoothness parameter $s=1$ for functions on the reals; when $s>1$  the dimensionality $d$ in the theorems should be replaced by $\frac{d}{s}$. 
% In the following we assume a smooth version of the RELU activation function.  A composition can be summarized by a directed acyclic graph (DAG) (see Figure~\ref{DAG}).
% If the relevant DAG is $\mathcal G$, we say the computed function is a compositional $\mathcal{G}$-function, and $\mathcal{G}$ is one of its compositional graph representations.

% \begin{theorem}[Adapted from \cite{MhaskarPoggio2016b} and \cite{Pinkus_1999, DBLP:journals/corr/abs-2201-09418}] Let $\mathcal{G}$ be a Directed Acyclic Graph ($D A G),  d$ be the number of source nodes, and for each $v \in V$, let $d_v$ be the number of in-edges of $v$. Let $f: \mathbb{R}^d \mapsto \mathbb{R}$ be a compositional $\mathcal{G}$-function, where each of the constituent functions is in the Sobolev space $\mathrm{W}_{s_v}^{d_v}$. Consider shallow and deep networks with infinitely smooth activation function. Then, the complexity (number of parameters) of the best approximating shallow network is exponential in $d$, that is $N_{shallow}=\mathcal{O}\left(\epsilon^{-\frac{d}{s}}\right)$,
% where $s=\min _{v \in V} s_v$, while the complexity of the deep network is $N_{deep}=\mathcal{O}(\sum_{v \in V} \epsilon^{-d_v / s_v})$.  Upper and lower bounds  on the  error in the $L^2$ norm of  shallow networks with $N$ RELU-like units approximating  target functions in $\mathrm{W}_{s}^{d}$ are both of the form  $c N^{-s /(d-1)}$, 
% with different constants $c$ independent of $N$. Another upper bound  in the sup norm wrt Hoelder target functions is $K^{-{\frac{s}{d+1}}}$, where $K=\Pi_{l=1}^L \|W_l\|$, where $\|\cdot\|$ is the operator norm.
% \label{DeepApproxTheorem}
% \end{theorem}


% The theorem implies that, if a representation of $f$ as a compositional $\mathcal G$ function is sparse, there exists a deep network - with an associated graph  corresponding to the graph of $f$ -- that avoids the curse of dimensionality in approximating $f$, whereas shallow networks cannot directly avoid the curse
% \footnote{
% % Observe that since representations of a function are not unique, a representation may not be sparse whereas another, for example using Fourier transforms, may be.
% Functions may be compositionally sparse in one domain (Fourier coefficients for e.g.), and may not be sparse in another domain.}.The obvious next question is whether sparse compositional functions are special. To answer we use a standard definition of {\it efficiently computable} functions\footnote{
% % \vspace{-0.5cm}
% % \begin{definition}
% % \label{Definition12}
%  {\it A function $f: I \rightarrow \mathbb{R}_c^k, I \subset \mathbb{R}_c^d$, where $\mathbb{R}_c$ is the set of computable real numbers, is called Borel-Turing computable, if there exists an algorithm (or Turing machine) that transforms each given computable representation of a  vector $x \in I$ -- for instance using rational numbers --  into a representation for $f(x)$. The special case of efficient computability requires computability in polynomial time/space in $d$.}
% % \end{definition}
% }.
% The following theorem summarizes several results from \cite{poggiofraser2024}:


% \begin{theorem}
%  Functions on $I \subset \mathbb{R}^d$ with Lipschitz continuity which are efficiently computable are compositionally sparse. Efficiently computable Boolean functions are compositionally sparse. For every efficiently computable function, there exists a deep, sparse network, matching the DAG of the function that approximates it without curse of dimensionality.
% \label{Theoremx}
% \end{theorem}
% %\footnote{It is useful to recall that programs that run in finite time are made of a finite number of intermediate functions and intermediate variables and that the dependencies between functions and variables can be expressed using a directed acyclic graph (DAG).}.
% In summary, {\it deep sparse networks can efficiently (i.e. without an exponential number of parameters) approximate 
% arbitrarily well, with a non-exponential number of parameters, all functions that are efficiently Turing computable}. This argument justifies choosing our space $\mathcal{H}$ of parametric functions to be deep networks (that should be sparse since they are used to approximate a sparse compositional representation of the target function). Thus we chose ${\mathcal H}$ as the function space spanned by neural networks with $L$ layers.
% More precisely, the elements of $\mathcal{H}$ are 
% compositions $f_W: \R^d \rightarrow \R^q$ of the following form that alternate between left-multiplication by a matrix $W_k$ and application of a non-linear function $\sigma$
% \begin{displaymath}
% f_W \colon 
% % (x) = W_L \sigma \left( W_{L-1} \ldots \sigma \left( W_1 x \right) \ldots \right),
% \R^d \xrightarrow[]{W_1} \R^{d_1} \xrightarrow[]{\sigma} \R^{d_1} 
% \xrightarrow[]{W_2} \R^{d_2} \xrightarrow[]{\sigma} \R^{d_2}
% \cdots
% \R^{d_{L-2}} \xrightarrow[]{W_{L-1}} \R^{d_{L-1}} \xrightarrow[]{\sigma} \R^{d_{L-1}} 
% \xrightarrow[]{W_L} \R^{q}.
% \label{DefDeepNet}
% \end{displaymath}
% finishing with left-multiplication by a matrix $W_L$. Here the input to the network is assumed to be 
% $x \in \R^d$; $W_k, k=1,\cdots,L$ are matrices; and the (abuse of) notation $\sigma: \R^j \rightarrow \R^j$
% denotes coordinate-wise application of a non-linear function. 
% Each function $f_W \in \mathcal{H}$ is thus specified by $L$ matrices of parameters $W_k, k=1,\cdots,L$ of suitable dimensions and the ERM is done in the space of these parameters. 
% %\footnote{The rectified linear unit (ReLU) activation function
% %$\sigma: \R \rightarrow \R$, $\sigma(x) = \textrm{max}(0,x)$ is commonly used, though recently the RELU has been replaced in applications by smooth versions such as the GELU activation function\cite{hendrycks2016gaussian}).}.

% \end{document}